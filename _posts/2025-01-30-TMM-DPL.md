---
title: "恭喜刘攀同学的研究工作DPL被TMM录用!
DPL has been accepted by TMM!"
date: 2025-01-30T15:34:30-04:00
categories:
  - Paper
tags:
  - Journal
  - TMM
---

恭喜刘攀同学的研究工作被IEEE TMM期刊录用。刘攀同学是2023级硕士研究生，由赵萌老师和我共同指导，此次被录用的论文是他攻读硕士学位的第一项研究工作，研究主题是无源域适应。TMM是IEEE在多媒体、信号处理、人工智能方向的旗舰级期刊，中科院期刊分区表一区期刊。

以下为该论文摘要：
随着对隐私和可移植性的担忧日益增加，无源域自适应仅需要一个源域预训练模型和一个无标签的目标域，就能实现对目标数据的有效适配。现有的大多数自训练方法都侧重于选择和利用具有可靠预测的样本，却常常忽略其他样本。鉴于有研究发现深度模型对干净样本的学习速度比对含噪样本更快，我们提出了一种基于域划分的渐进式学习方法，名为DPL。具体而言，我们的方法包含两个交替阶段。每个阶段都先根据适配难度，将目标域划分为易于适配和难以适配的子域，随后基于邻域进行伪标签分配。在第一阶段，我们通过不确定性感知自训练以及子域间对应类别的对齐来提高分类准确率。第二阶段则针对每个子域应用定制的学习策略，先对易于适配的样本进行一致性学习，再对更具挑战性的样本利用局部结构信息，从而挖掘目标数据的内在特性。
在多个广泛使用的基准数据集上进行的大量实验验证了我们方法的有效性，结果表明，与当前最先进的方法相比，我们的方法表现更为出色。 

Congratulations to Liu Pan whose research work has been accepted by IEEE TMM. Liu Pan is a master student co-guided by Prof. Meng Zhao and me since September, 2023. The paper accepted is his first research work for his master's degree, whose research topic is source-free domain adaptation. IEEE TMM is a flagship journal of IEEE in the fields of multimedia, signal processing, and artificial intelligence, and it is also a Q1 journal in the Journal Ranking of the Chinese Academy of Sciences. The abstract of this paper is given as follows:

With growing privacy and portability concerns, source-free domain adaptation requires only a source pre-trained model and an unlabeled target domain, allowing for effective adaptation to the target data. Most existing self-training methods focus on selecting and exploiting samples with reliable predictions, often neglecting others. Inspired by the finding that deep models learn clean samples faster than noisy ones, we propose a domain-division based progressive learning method named DPL. Specifically, our approach consists of two alternating stages, each beginning with the division of the target domain into easy-to-adapt and hard-to-adapt subdomains based on adaptation difficulty, followed by neighborhood-based pseudo label assignment. In stage one, we enhance classification accuracy through uncertainty-aware self-training and alignment of corresponding classes between subdomains. Stage two then applies tailored learning strategies to each subdomain, starting with consistency learning on the easy-to-adapt samples and progressing to utilizing local structural information for the more challenging ones, thereby mining the intrinsic properties of the target data. Extensive experiments on several widely used benchmarks validate the effectiveness of our approach, demonstrating superior performance compared to state-of-the-art methods.
